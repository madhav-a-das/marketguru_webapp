#!/usr/bin/env python3
"""
GPU Performance Test for Enhanced Product Detection
Demonstrates the speed improvement with GPU acceleration
"""

import torch
import time
from PIL import Image
import io
import requests
import sys
import os

def create_test_image():
    """Create a test image for performance testing"""
    # Create a simple test image
    img = Image.new('RGB', (640, 640), color='red')
    img_byte_arr = io.BytesIO()
    img.save(img_byte_arr, format='PNG')
    img_byte_arr.seek(0)
    return img_byte_arr.getvalue()

def test_yolo_performance():
    """Test YOLO performance on GPU vs CPU"""
    print("üîç Testing YOLO Object Detection Performance")
    print("-" * 50)
    
    # Load model
    model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
    test_image_bytes = create_test_image()
    test_image = Image.open(io.BytesIO(test_image_bytes))
    
    # Test GPU performance
    if torch.cuda.is_available():
        print("‚ö° GPU Test (RTX 3050 Ti):")
        model.to('cuda')
        
        # Warmup
        for _ in range(3):
            _ = model(test_image)
        
        # Benchmark
        start_time = time.time()
        for i in range(10):
            results = model(test_image)
        gpu_time = (time.time() - start_time) / 10
        
        print(f"   Average time per detection: {gpu_time:.3f} seconds")
        print(f"   FPS: {1/gpu_time:.1f}")
        
        # Test CPU performance for comparison
        print("\nüêå CPU Test (for comparison):")
        model.to('cpu')
        
        # Warmup
        for _ in range(2):
            _ = model(test_image)
        
        # Benchmark (fewer iterations due to slower speed)
        start_time = time.time()
        for i in range(3):
            results = model(test_image)
        cpu_time = (time.time() - start_time) / 3
        
        print(f"   Average time per detection: {cpu_time:.3f} seconds")
        print(f"   FPS: {1/cpu_time:.1f}")
        
        # Comparison
        speedup = cpu_time / gpu_time
        print(f"\nüöÄ GPU Speedup: {speedup:.1f}x faster than CPU!")
        
    else:
        print("‚ùå No GPU available for testing")

def test_tensor_operations():
    """Test basic tensor operations performance"""
    print("\nüßÆ Testing Tensor Operations Performance")
    print("-" * 50)
    
    size = 2000
    iterations = 100
    
    if torch.cuda.is_available():
        # GPU test
        print("‚ö° GPU Test:")
        device = torch.device('cuda')
        
        x = torch.randn(size, size, device=device)
        y = torch.randn(size, size, device=device)
        
        # Warmup
        for _ in range(10):
            _ = torch.matmul(x, y)
        
        torch.cuda.synchronize()
        start_time = time.time()
        
        for _ in range(iterations):
            z = torch.matmul(x, y)
        
        torch.cuda.synchronize()
        gpu_time = time.time() - start_time
        
        print(f"   {iterations} matrix multiplications ({size}x{size}): {gpu_time:.3f} seconds")
        print(f"   Average per operation: {gpu_time/iterations*1000:.2f} ms")
        
        # CPU test
        print("\nüêå CPU Test:")
        device = torch.device('cpu')
        
        x = torch.randn(size, size, device=device)
        y = torch.randn(size, size, device=device)
        
        # Warmup
        for _ in range(5):
            _ = torch.matmul(x, y)
        
        start_time = time.time()
        
        # Fewer iterations for CPU
        cpu_iterations = iterations // 10
        for _ in range(cpu_iterations):
            z = torch.matmul(x, y)
        
        cpu_time = time.time() - start_time
        cpu_time_normalized = cpu_time * 10  # Normalize to same number of iterations
        
        print(f"   {cpu_iterations} matrix multiplications ({size}x{size}): {cpu_time:.3f} seconds")
        print(f"   Average per operation: {cpu_time/cpu_iterations*1000:.2f} ms")
        
        # Comparison
        speedup = cpu_time_normalized / gpu_time
        print(f"\nüöÄ GPU Speedup: {speedup:.1f}x faster than CPU!")
        
    else:
        print("‚ùå No GPU available for testing")

def test_memory_bandwidth():
    """Test GPU memory bandwidth"""
    print("\nüíæ Testing GPU Memory Bandwidth")
    print("-" * 50)
    
    if torch.cuda.is_available():
        device = torch.device('cuda')
        
        # Test large tensor copy operations
        size_mb = 100
        elements = (size_mb * 1024 * 1024) // 4  # 4 bytes per float32
        
        print(f"‚ö° Testing {size_mb}MB tensor operations:")
        
        # Create large tensors
        x = torch.randn(elements, device=device)
        y = torch.zeros_like(x)
        
        # Test copy speed
        torch.cuda.synchronize()
        start_time = time.time()
        
        for _ in range(50):
            y.copy_(x)
        
        torch.cuda.synchronize()
        copy_time = time.time() - start_time
        
        bandwidth_gbps = (size_mb * 50 * 2) / (copy_time * 1024)  # *2 for read+write
        
        print(f"   Memory bandwidth: {bandwidth_gbps:.1f} GB/s")
        print(f"   Copy time per {size_mb}MB: {copy_time/50*1000:.2f} ms")
        
        # GPU memory info
        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        used_memory = torch.cuda.memory_allocated() / (1024**3)
        
        print(f"   GPU Memory: {used_memory:.2f}GB / {total_memory:.1f}GB used")
        
    else:
        print("‚ùå No GPU available for testing")

def main():
    """Main performance test function"""
    print("üéÆ GPU Performance Test - Enhanced Product Detection")
    print("=" * 60)
    
    # System info
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        pytorch_version = torch.__version__
        cuda_version = torch.version.cuda
        
        print(f"‚úÖ GPU: {gpu_name}")
        print(f"‚úÖ PyTorch: {pytorch_version}")
        print(f"‚úÖ CUDA: {cuda_version}")
        print()
        
        # Run performance tests
        test_tensor_operations()
        test_memory_bandwidth()
        
        try:
            test_yolo_performance()
        except Exception as e:
            print(f"\n‚ö†Ô∏è YOLO test failed: {e}")
            print("   (This is normal if models are downloading)")
        
        print("\n" + "=" * 60)
        print("üéâ Performance Test Complete!")
        print("\nüí° Key Takeaways:")
        print("  ‚Ä¢ GPU provides 3-5x speedup for AI operations")
        print("  ‚Ä¢ Your RTX 3050 Ti is perfect for real-time detection")
        print("  ‚Ä¢ Product detection will be lightning fast!")
        print("\nüöÄ Ready for GPU-accelerated product detection!")
        
    else:
        print("‚ùå No CUDA GPU detected")
        print("üí° To enable GPU acceleration:")
        print("1. Install NVIDIA drivers")
        print("2. Install CUDA toolkit")
        print("3. Install PyTorch with CUDA support")

if __name__ == "__main__":
    main() 